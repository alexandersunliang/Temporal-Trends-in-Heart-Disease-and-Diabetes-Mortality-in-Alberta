---
title: "Temporal Trends in Heart Disease and Diabetes Mortality in Alberta: A comparison between Poisson and Negative Binomial techniques"
author: 
  - "Alexander Sun"
thanks: "https://github.com/alexandersunliang/Temporal-Trends-in-Heart-Disease-and-Diabetes-Mortality-in-Alberta.git"
date: today
date-format: long
abstract: "In this study, we analyzed mortality data from Alberta, Canada, focusing on the effects of heart disease and diabetes from 2016 to 2021, amidst increasing fast food consumption. Our findings, using Poisson and negative binomial regression models, indicate that while heart disease is positively associated with mortality rates, diabetes, and other cardiovascular conditions are linked to lower mortality rates relative to the baseline, suggesting effective management of these conditions over time. This research illustrates the complex relationship between diseases and health outcomes, highlighting balance between medical advancement and deteriorating dietary habits. By shedding light on these temporal trends, our study contributes to a broader understanding of how diet influences the prevalence and mortality rates of heart disease and diabetes, reinforcing the importance of dietary education and regulation in public health initiatives."
format: pdf
toc: true
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(knitr)
library(kableExtra)
library(here)
library(dplyr)
library(bayesplot)
library(rstanarm)
library(gridExtra)
library(ggplot2)
library(modelsummary)
library(broom.mixed)
filtered_data <- read.csv(here("data/analysis_data/analysis_data.csv"))
```

\newpage

# Introduction {#sec-intro}
 

In recent decades, the prevalence of heart disease and diabetes has surged globally, a trend paralleled by the increasing consumption of fast food [@heartdisease]. This paper focuses on Alberta, Canada, where these health challenges have become particularly pronounced. While numerous studies have linked fast food consumption to various health outcomes, few have directly examined how mortality rates from heart disease and diabetes have changed within this region. This gap in research motivates our study, which aims to analyze trends in these mortality rates and discuss their potential association with the rise in fast food consumption, despite the absence of direct consumption data.

Using mortality data from the Alberta government, spanning two decades, we applied both Poisson and negative binomial regression models to analyze changes in heart disease and diabetes mortality rates [@Alberta]. Our findings reveal significant trends in these rates, with notable increases that correspond to periods of reported national and global rises in fast food consumption and COVID-19. While direct causation cannot be established due to the lack of specific consumption data, the correlations emphasize the potential health impacts of dietary habits.

The importance of this research lies in its contribution to the ongoing dialogue about public health strategies aimed at combating heart disease and diabetes. By highlighting the temporal association between increased mortality rates and the era of rising fast food consumption, this study emphasizes the need for targeted public health interventions and policies. Furthermore, by focusing on the past five years, we can further investigate the potential correlation between COVID-19 and heart-related diseases/diabetes.

This paper is organized as follows: Following the introduction, the second section provides an overview of our dataset, variables, and a general analysis of the data, establishing the theoretical foundation for the study. The third section describes the model, including the rationale behind the choice of negative binomial regression. The fourth section presents our findings, detailing the effects of our various illnesses on mortality rates. The fifth section discusses the implications of these findings for public health policy and suggests directions for future research. For this study, the estimand is the effect of heart diseases and diabetes on mortality rates.

Our goal is to provide an insight into how large macro-events from the past few years may have changed the mortality rates of diseases correlated with unhealthy eating.


# Data {#sec-data}

## Data source
This analysis will be carried out in R [@citeR] using packages tidyverse [@tidyverse], rstanarm [@Rstanarm], dplyr [@Dplyr], ggplot2 [@Ggplot2], knitr [@Knitr], gridextra[@gridextra], bayesplot[@bayesplot], and modelsummary [@modelsum]. The data set used in this paper is called *Leading causes of death* and was collected from the Alberta Provincial Government [@Alberta]. The data set consists of a ranking of the 30 most common causes of death each year in Alberta. The government collected this data using death certificates from all over the province, making the information reasonably reliable and accurate. The data covers the last two decades, but for our research purpose we will focus on the last five years.

## Broader Context of the Dataset
The availability of detailed public health data, such as the mortality statistics from Alberta, is crucial for the formulation of informed public health policies and strategies. Within the broader Canadian context, Alberta's commitment to data transparency enables a deeper analysis of health trends and outcomes, serving as a model for other provinces and territories. The analysis of mortality data plays a pivotal role in identifying health trends, assessing the burden of diseases, and planning public health interventions. By focusing on specific causes of death, researchers and policymakers can tailor strategies to target the underlying factors contributing to these trends, ultimately aiming to improve health outcomes and reduce preventable deaths.

## Variables
The dataset comprises several key variables, central to this study's focus on heart disease and diabetes mortality rates:

Causes of Death: Specifically, the dataset categorizes mortality into detailed causes, including:

- **All Other Forms of Ischemic Heart Disease**: This category encompasses various conditions related to reduced blood flow to the heart muscle, excluding acute myocardial infarction.
- **Acute Myocardial Infarction (Heart Attack)**: Fatalities resulting directly from heart attack incidents.
- **Atherosclerotic Cardiovascular Disease**: Deaths caused by atherosclerosis, a condition characterized by the hardening and narrowing of the arteries due to plaque buildup, leading to cardiovascular problems.
- **Diabetes Mellitus**: Mortality attributed to complications arising from diabetes, a chronic condition affecting blood sugar regulation.
- **Congestive Heart Failure**: Deaths resulting from the heart's inability to pump blood effectively, often a consequence of other heart conditions.

These five causes of death were chosen due to their correlation with unhealthy diets. We collected the recorded total deaths in Alberta from each of the above variables from 2016-2021 inclusive. This is shown in @tbl-cause.

```{r}
#| echo: false
#| tbl-cap: Yearly death counts for various heart diseases and diabetes
#| label: tbl-cause
# Creating a tibble consisting of our information and names. Could have done this with the filtered_data we generated but it was bugging out for some reason so just decided to do it manually
summary_table <- tibble(
  Year = rep(2016:2021, times = 5),
  Cause_of_Death = rep(c("Acute Myocardial Infarction",
                         "All Other Forms of Ischemic Heart Disease",
                         "Atherosclerotic Cardiovascular Disease",
                         "Diabetes Mellitus",
                         "Congestive Heart Failure"), each = 6),
  Total_Deaths = c(1102, 1028, 1071, 1061, 1067, 1075,       
                   1626, 1678, 1788, 1886, 1897, 1939,  
                   885, 817, 630, 745, 678, 463,       
                   502, 584, 577, 569, 743, 728, 
                   352, 374, 347, 430, 387, 403)       
) %>%
  pivot_wider(names_from = Cause_of_Death, values_from = Total_Deaths) %>%
  select(Year, "Acute Myocardial Infarction", "All Other Forms of Ischemic Heart Disease",
         "Atherosclerotic Cardiovascular Disease", "Diabetes Mellitus", "Congestive Heart Failure")
kable(summary_table, format = "latex", booktabs = TRUE,) %>%
  kable_styling(latex_options = c("striped", "scale_down"))
# Generating the table
```


## Data Preparation and Cleaning
The dataset was filtered to isolate deaths attributed to our causes of interest: acute myocardial infarction, all other forms of ischemic heart disease, atherosclerotic cardiovascular disease, diabetes mellitus, and congestive heart failure. This selection was crucial to align our study with its objectives, ensuring a focused examination of these specific health outcomes. Subsequently, we removed all missing values from the dataset. The final data points were compiled for use in our analysis later in the paper.

## Preliminary Observations and Exploration
Our initial analysis revealed several notable observations. First, the trend analysis suggested a correlation between the years and mortality rates for specific causes of death, hinting at the possible influence of external factors such as healthcare policies or changes in societal health behaviors. Since we are looking at a five-year span from 2016 to 2021, the onset of the COVID-19 pandemic introduced a plethora of health-related policies and regulations that may significantly impact our data. In @fig-overview-line, we plotted the total deaths from each of the causes to visualize if there were any spikes in the data.
```{r}
#| echo: false
#| label: fig-overview-line
#| fig-cap: Trends in Total Deaths per Year by Cause from 2016-2021
library(tidyverse)

# Plotting filtered_data which consists of all our data points after the cleaning
ggplot(filtered_data, aes(x = calendar_year, y = total_deaths, color = cause, group = cause)) +
  geom_line() + # Draw lines
  geom_point() + 
  theme_minimal() +
  labs(
    title = "Trends in Total Deaths by Cause (2016-2021)",
    x = "Calendar Year",
    y = "Total Deaths Per Year",
    color = "Cause of Death"
  ) +
  scale_x_continuous(breaks = seq(min(filtered_data$calendar_year), max(filtered_data$calendar_year), by = 1)) # Ensure every year is shown
```
From @fig-overview-line, we see that although most causes kept their total death count per year roughly the same, Diabetes saw a noticeable spike form 2020 onwards and Atheroscleratic cardiovascular disease saw a significant decrease. 

# Model {#sec-model}

In the realm of statistical modeling for count data, both the Poisson and Negative Binomial models are popular choices, each with its assumptions, benefits, and drawbacks. These models serve to elucidate the relationship between a set of predictors—in this case, the causes of death—and the count response variable, which is the total number of deaths per year.

The Poisson regression model is based on the assumption that the count of events follows a Poisson distribution, meaning its mean is equal to its variance. This assumption is suitable for datasets where the occurrence of events is rare and independent over a fixed period or space, such as the number of deaths in a small population or rare diseases. The simplicity of the Poisson model lies in its single parameter, making it computationally efficient and easy to interpret. However, this simplicity also constitutes a limitation, particularly when it comes to overdispersion—a scenario often encountered in real-world data where the variance exceeds the mean due to heterogeneity in the data. When overdispersion is present, the Poisson model can underestimate the standard errors of the estimated coefficients, leading to an overstatement of the statistical significance of predictors.

In contrast, the Negative Binomial model adds an extra parameter to account for overdispersion, offering a more flexible fit for count data that exhibit greater variability. This model is especially apt when the data are counts of events that can occur more frequently than a rare event and when these counts have high variance. It can handle data from populations with heterogeneity that the Poisson model cannot accommodate. This flexibility comes with a trade-off in terms of increased model complexity and computational demand. The added dispersion parameter means that the Negative Binomial model has one more degree of freedom than the Poisson model, which can result in a better fit but also requires careful interpretation of the additional parameter.

For the current paper focusing on mortality rates due to diabetes and heart disease, it is imperative to choose a model that accurately reflects the underlying data distribution. If the mortality counts are subject to overdispersion, the Negative Binomial model would likely provide a more accurate representation of the data, enabling more reliable statistical inferences. For instance, if certain causes of death are more prevalent or exhibit more variability from year to year, the Negative Binomial model's capacity to incorporate this variability would make it a superior choice over the Poisson model. Ultimately, the selection between the two models should be guided by diagnostic checks, such as the dispersion parameter and goodness-of-fit tests.

## Poisson Model
Listed below is what an example Poisson model would look like if applied to our scenario.
$$
y_i | \lambda_i \sim \text{Poisson}(\lambda_i)
$$
$$
\log(\lambda_i) = \alpha + \beta_1 \times \text{AMI}_i + \beta_2 \times \text{OtherIHD}_i + \beta_3 \times \text{ASCVD}_i + \beta_4 \times \text{Diabetes}_i + \beta_5 \times \text{CHF}_i
$$
where:

- $y_i$ is the count of deaths due to the $i$-th cause.
- $\lambda_i$ is the expected count of deaths for the $i$-th cause.
- $\alpha$, $\beta_1$, $\beta_2$, $\beta_3$, $\beta_4$, and $\beta_5$ are the model parameters to be estimated.
- AMI, OtherIHD, ASCVD, Diabetes, and CHF are abbreviations of our previously selected variables.

## Negative Binomial Model
To address potential overdispersion in our data, we also applied the Negative Binomial regression model. This model extends the Poisson by introducing an extra parameter to account for the overdispersion, offering a more flexible approach to fit our data. The formulation of the Negative Binomial model is:

\begin{align}
y_i | \mu_i, \phi &\sim \text{NegBin}(\mu_i, \phi) \\
\mu_i &= \exp(\alpha + \beta x_i) \\
\alpha &\sim \text{Normal}(0, 2.5) \\
\beta &\sim \text{Normal}(0, 2.5) \\
\phi &\sim \text{Exponential}(1)
\end{align}

In this model, $y_i$ denotes the total number of deaths per year, with $\mu_i$ being the expected count adjusted for overdispersion through the dispersion parameter $\theta$. $\beta$_{0} represents the intercept and $\beta$_{i} represents the effects of a unique cause of death. The value and sign of $\beta$ denote whether having that disease/sickness increase or decrease the death count of that year and by how much.

Given the observed overdispersion in our dataset, the Negative Binomial model is anticipated to offer a more accurate and reliable fit compared to the Poisson model. By incorporating the extra dispersion parameter, it allows us to better capture the variability in death counts across different causes, providing a nuanced understanding of how each cause contributes to overall mortality. In applying these models, we aim to discern the relative impact of specified causes of death on the total number of deaths, while also accounting for the distributional characteristics of our count data. Through this comparative analysis, we seek to identify the most suitable model for our dataset, thereby enhancing the reliability of our findings and conclusions.

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.


### Model justification

Since we have two potential methods of modeling, we can directly compare the two to decide which regression to run. In the previous section, we hypothesized that the Poisson model will lead to more inaccurate results due to overdispersion of data. We can see if the assumption that variance is equal to the mean is true for our $y_i$ value for total death count in that given year.

```{r}
#| echo: false
#| warning: false
#| label: tbl-sumcause
#| tbl-cap: "Summary Statistics of the Number of Yearly Deaths by Cause"
# Generating the summary statistics 
summary_stats <- filtered_data %>%
  summarise(
    Min = min(total_deaths),
    Mean = mean(total_deaths),
    Max = max(total_deaths),
    SD = sd(total_deaths),
    Var = var(total_deaths),
    N = n()
  )
# Generating the kable table
kable(summary_stats, 
      col.names = c("Minimum", "Mean", "Maximum", "Standard Deviation", "Variance", "Count"), 
      align = 'c') %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```
From @tbl-sumcause, we see that the variance is significantly off of the mean. Therefore, the primary assumption for applying a Poisson model where the variance is equal to the mean does not hold. From this alone, we can reasonably conclude that a negative binomial model most likely fits our purpose of estimating the total death count per year better due to accounting for overdispersion with the $\mu_i$ variable. 
```{r}
#| echo: false
#| message: false
#| warning: false

# Generating the regressions for Poisson and negative binomial

cause_of_death_poisson <- stan_glm(
  total_deaths ~ cause,
  data = filtered_data,
  family = poisson(link = "log"),
  seed = 853,
  refresh = 0
)

#Comparing the graphs
cause_of_death_neg_binomial <- stan_glm(
  total_deaths ~ cause,
  data = filtered_data,
  family = neg_binomial_2(link = "log"),
  seed = 853,
  refresh = 0
)
```
# Results {#sec-results}

```{r}
#| echo: false
#| warning: false
#| tbl-cap: "Regression coefficients for Poisson and Negative Binomial"
#| label: tbl-regression
poisson_summary <- tidy(cause_of_death_poisson)
negbin_summary <- tidy(cause_of_death_neg_binomial)

combined_results <- bind_rows(
  mutate(poisson_summary, Model = "Poisson"),
  mutate(negbin_summary, Model = "Negative Binomial")
)

# Use modelsummary to create the comparison table
modelsummary(
  list(Poisson = cause_of_death_poisson, `Negative Binomial` = cause_of_death_neg_binomial),
  output = "kableExtra"
)
```

@tbl-regression presents a comparative summary of the regression results obtained from the Poisson and Negative Binomial models. The comparison is based on a dataset of mortality rates due to various causes of heart disease and diabetes. The table provides estimates of the regression coefficients, along with their standard errors (in parentheses), for each cause of death, as well as several model diagnostics.

**Intercept:** Both models show a large positive intercept, implying a high baseline number of deaths when all other variables are at zero. This suggests that there are other factors contributing to mortality rates that are not included in the model.

**Coefficient Estimates:** The coefficients represent the log change in the expected count of total deaths for each cause of death. For instance, "All Other Forms of Ischemic Heart Disease" has a positive coefficient in both models, suggesting a higher mortality rate associated with this cause. "Atherosclerotic Cardiovascular Disease" and "Diabetes Mellitus" have negative coefficients, indicating a lower count of deaths relative to the baseline for these causes.

**Standard Errors:** The standard errors provide an indication of the precision of the coefficient estimates. The Negative Binomial model typically shows larger standard errors than the Poisson, reflecting additional uncertainty due to modeling overdispersion.

**Num.Obs.** indicates the number of observations used in both models, which is the same, providing a consistent basis for comparison.

**Log.Lik. (Log-Likelihood)** measures the fit of the model to the data, with higher values indicating a better fit. The Negative Binomial model shows a less negative log-likelihood, suggesting a better fit to the data compared to the Poisson model.

**ELPD (Expected Log Predictive Density)** and its standard error are measures for model comparison and predictive performance, where higher values indicate a better model. The Negative Binomial model has a higher ELPD, indicating better predictive performance.

**LOOIC (Leave-One-Out Information Criterion)** is similar to AIC but based on leave-one-out cross-validation; lower values indicate a better model fit. The Negative Binomial model has a lower LOOIC, suggesting a better fit.

**WAIC (Watanabe-Akaike Information Criterion)** is another model selection criterion that is fully Bayesian and accounts for model complexity; like AIC and LOOIC, lower is better. Again, the Negative Binomial model performs better.

**RMSE (Root Mean Square Error)** reflects the model's accuracy in predicting the number of deaths, with lower values indicating a more accurate model. Both models show similar RMSE, indicating comparable predictive accuracy.


From the table above, we have the following calculated regression models using both Poisson and Negative Binomial:

\textbf{Poisson Regression Model:}
\begin{equation}
\log(\lambda_i) = 6.973 + 0.524 \cdot X_{\text{Ischemic Heart Disease}} - 0.408 \cdot X_{\text{Cardiovascular Disease}} - 1.028 \cdot X_{\text{Heart Failure}} - 0.548 \cdot X_{\text{Diabetes}}
\end{equation}

\textbf{Negative Binomial Regression Model:}
\begin{equation}
\log(\mu_i) = 6.976 + 0.528 \cdot X_{\text{Ischemic Heart Disease}} - 0.414 \cdot X_{\text{Cardiovascular Disease}} - 1.023 \cdot X_{\text{Heart Failure}} - 0.547 \cdot X_{\text{Diabetes}}
\end{equation}

# Discussion {#sec-discussion}

## Interpretation of the regression models

Our regression analysis revealed several intriguing patterns about the relationship between various health conditions and mortality rates. In interpreting the models, it is crucial to consider the baseline level of deaths, represented by the intercept, which is the expected number of deaths when the incidence of specific conditions is zero. For the Poisson and Negative Binomial models, the intercepts were approximately 6.973 and 6.976, respectively. When exponentiated, these values suggest a high baseline number of deaths, which serves as a point of comparison for the other coefficients in the model. It is important to clarify that a positive coefficient for a health condition does not imply that the presence of the condition increases one's likelihood of survival. Instead, it reflects the relative risk of mortality associated with that condition compared to the baseline established by the intercept. For example, the positive coefficient for "All Other Forms of Ischemic Heart Disease" does not indicate that the disease leads to improved survival but rather that it is associated with a higher mortality rate relative to the baseline level of deaths captured by the intercept.

The negative coefficients for conditions like "Atherosclerotic Cardiovascular Disease" and "Diabetes Mellitus" in both models indicate that, relative to the intercept, these conditions are associated with a lower mortality rate. This seemingly paradoxical result could be due to effective management and treatment strategies that improve survival rates for these conditions. Moreover, it might reflect the relative impact of these conditions compared to more lethal alternatives that define the intercept's baseline mortality. "Congestive Heart Failure" showed a negative association with mortality in the model. This finding suggests that, while heart failure is a serious condition, its presence as a recorded cause of death has a lower relative risk compared to the baseline mortality rate, which could include deaths from more acute or untreated conditions.

The differences in coefficients between the two models can provide insights into model fit and appropriateness. The Negative Binomial model generally yielded a better fit to our data, indicated by less negative log-likelihood, higher expected log predictive density (ELPD), and lower information criteria (LOOIC and WAIC) scores. This suggests that it more accurately accounts for variability in mortality data, potentially due the variance in the data being higher than the mean, which is common in count data like mortality statistics.

## Model Comparison

The log-likelihood values provide an indication of how well the model fits the data. For the Poisson model, the log-likelihood is -274.969, while for the Negative Binomial model, it is considerably higher (less negative) at -196.422. This substantial difference suggests that the Negative Binomial model provides a much better fit to the data compared to the Poisson model. A higher log-likelihood (closer to zero) implies that the model's estimated probabilities of the observed outcomes are higher, meaning that the model is more likely to produce the observed data.Furthermore, the LOOIC values are another essential tool for model comparison. They offer a balance between model fit and complexity, penalizing for overfitting. For the Poisson model, the LOOIC is 609.7, with a standard error of 121.7. In contrast, the Negative Binomial model has a LOOIC of 396.5, with a much smaller standard error of 5.8. The lower LOOIC value for the Negative Binomial model, combined with its lower standard error, indicates a model that is more efficient in explaining the data without overfitting.

The graphs presented in @fig-comp illustrate the posterior predictive checks for the Poisson and Negative Binomial models applied to the mortality data concerning various causes of death. For both graphs, the x-axis represents the count of total deaths, and there are two sets of lines: one for the observed counts (denoted by y) and another for the predicted counts from the respective models (denoted by y_rep). The lines for predicted counts represent different simulated datasets generated by the model, reflecting its probabilistic predictions.

In the Poisson Model graph, the overlapping lines suggest that while the model captures the general trend of the observed data, it may not encapsulate the full variability, as indicated by the divergence between the observed and predicted counts particularly in regions of higher death counts. The model seems to generally underpredict the total death count per year when the data peaks which could be caused by the difference in variance and mean. 

Conversely, in the Negative Binomial Model graph, the predicted counts lines appear to more closely follow the pattern of the observed data, including the peaks and valleys, which indicates a better fit. This model's ability to account for overdispersion with its $\mu_i$ term likely contributes to its more accurate representation of the variability in the data. The thicker lines indicating the observed counts y remain consistent across both models, serving as a benchmark for evaluating the predictive accuracy. The spread and concentration of the lighter lines (the predictive simulations y_rep) around these observed counts provide visual insight into the model's performance.

The Negative Binomial model appears to yield a better fit to the data, particularly for larger counts, which suggests it might be the more appropriate model for analyzing the impact of diabetes and heart disease on mortality rates within the studied population. This would be consistent with the expectation that mortality data, influenced by complex and varied factors, would exhibit overdispersion—an assumption better accommodated by the Negative Binomial model than by the Poisson model. 


```{r}
#| echo: false
#| label: fig-comp
#| fig-cap: Comparison of Poisson and Negative Binomial Models

# Generating the comparison graphs between the two models
p1 <- pp_check(cause_of_death_poisson) + 
  ggtitle("Poisson Model") +
  theme(legend.position = "bottom")

p2 <- pp_check(cause_of_death_neg_binomial) + 
  ggtitle("Negative Binomial Model") +
  theme(legend.position = "bottom")

gridExtra::grid.arrange(p1, p2, ncol = 2)
```


## Impact of COVID-19

The onset of the COVID-19 pandemic in late 2019 brought unprecedented changes to global health trends. Its direct impact on mortality rates would likely be reflected in the data for the years 2020 and 2021. If COVID-19 is included as a variable in the regression model, we might observe a significant coefficient associated with it, reflecting its substantial effect on mortality rates. Moreover, the pandemic could indirectly affect the coefficients of other variables. Healthcare systems were overwhelmed, which could have led to an increase in mortality from other causes due to resource reallocation. The model's intercept in the years of the pandemic might capture a higher baseline mortality rate due to the surge in deaths associated with COVID-19. Although illnesses directly related to COVID-19 were not included in the regression models, it is entirely possible that patients that contracted COVID-19 had weaker immune systems, and as a result increased the mortality even though their cause of death was not COVID-19. From @fig-overview-line, we see that there is a significant uptick in Diabetes and Ischemic Heart Disease following 2019 when the pandemic began. This may be the result of either a lack of available hospital care due to medical services being overloaded with the pandemic, an increase in fast food consumption due to quarantine, or other causes brought about by COVID. 

## Weaknesses and next steps

A notable weakness of the study lies in its indirect measure of fast food consumption. The lack of direct dietary data necessitates reliance on broader consumption trends as a proxy, potentially obscuring the nuanced relationship between individual dietary habits and health outcomes. This gap highlights the need for comprehensive datasets that encompass dietary patterns, enabling a more granular analysis of the impact of specific dietary choices on health.

Furthermore, the regression analysis, though insightful, is constrained by the scope of variables included. The absence of factors such as socioeconomic status, physical activity levels, and access to healthcare could result in an incomplete picture of the determinants of mortality rates. The coefficients indicating a decrease in mortality associated with certain health conditions may reflect advancements in treatment and management rather than a true decrease in risk, underscoring the importance of contextualizing statistical findings within the broader landscape of public health research.

Another potential limitation is the assumption of linearity in the relationship between the predictor variables and the log count of deaths. Real-world data, especially in the context of health outcomes, often exhibit complex, non-linear interactions that could be better captured through alternative modeling approaches. Exploring models that accommodate such non-linearities could provide deeper insights into the dynamics at play.

In terms of next steps, this research paves the way for several avenues of inquiry. Foremost among these is the integration of detailed dietary data into the analysis. Collaborations with public health agencies to access or collect granular consumption data could significantly enhance the study's ability to draw direct correlations between diet and mortality outcomes. Additionally, expanding the model to include a wider array of variables, such as lifestyle factors and healthcare access, would allow for a more comprehensive understanding of the influences on mortality rates.

Adopting a longitudinal approach to examine changes in individual health outcomes over time, potentially through cohort studies, could also address some of the limitations inherent in cross-sectional data analysis. Such studies would provide a richer dataset for examining the impact of fast food consumption on health, accounting for changes in dietary habits, healthcare practices, and disease prevalence over time.

Ultimately, this research contributes to an essential dialogue on the intersection of diet, public health, and mortality. By spotlighting the need for more nuanced data and sophisticated analytical techniques, it calls for a concerted effort among researchers, policymakers, and public health practitioners to address the complex challenges at the heart of diet-related health outcomes.


\newpage


# References {#sec-references}


